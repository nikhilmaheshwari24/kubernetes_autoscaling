# Manaul Scaling

## Course Overview

Let's talk about this course when it comes to Kubernetes autoscaling, also known as K8s autoscaling. Let's talk about it. Okay. So Kubernetes autoscaling, what are we focused on here? So first of all, welcome to this course, because we're going to talk all about Kubernetes and how to automate scaling. That is our primary focus. Now, there's a number of different facets to this, including cluster autoscaling and pod autoscaling. There's vertical and horizontal, there's event-driven, there's a bunch of different aspects to this. You know, imagine that you're running this online store, right? You're running an online store. And what's happening is that the store is getting a lot of traffic. Users are coming in, you're running a flash sale. And all of a sudden, the load on your application has gone right through the roof. Your servers start to buckle under the pressure, right? And if you've ever experienced this, it's not fun, right? It's like, what do you do? You need scaling, the website's going to slow down, customers can't access, they're having a bad experience. And you can almost hear the groan of frustration. It's not the outcome that you're looking for, right? So what's happening here is that autoscaling enters in as the hero of the day, because with it, your infrastructure basically automatically springs into action, automatically adjusting resources as needed up during high traffic and dialing it back when things are lower. It's like having a thermostat for your server load. And this is the magic of Kubernetes autoscaling is ensuring your applications remain resilient and responsive. Do you want to see how this works? Let's dive into the mechanics. 

Okay, so let's say you have a Kubernetes cluster, right? And this is really the heart. You've got a Kubernetes cluster, it's got worker nodes as part of the cluster. In addition to the data plane, which is not shown here, right? And each node is a cog in the scaling machine, and it is ready to take care of applications, provide access to capacity, traffic, resource usage, whatever. And this setup, by the way, helps prevent performance bottlenecks and downtime, because one of these worker nodes can fail, and another worker node can take it over. 

So what happens is that these worker nodes basically are going to scale and accommodate traffic as it rises up and down, providing more capacity to the cluster, but also potentially more capacity to the applications that are on the cluster. So it's this interestingly choreographed dance where both the worker nodes and the applications that sit on top of these worker nodes are going to scale up and down based on whether traffic is spiking or lower. 

Now, for this course, we're going to focus on a few key things here. One, we're going to talk to you a little bit about scaling, make sure that we're very clear, right, about `what we mean by scaling` and what we're looking for. We're then going to talk about the `Horizontal Pod Autoscaler`, which controls pod scaling, which is basically application scaling. So there are a couple of key pieces of this, but it can include `custom metrics`, `external metrics`, and `multiple metrics`. And so just know that this applies to all of the autoscalers to a certain degree. We also, by the way, have the `Vertical Pod Autoscaler`, which isn't necessarily going to expand the number of applications, but it's going to allow the applications access to more resources. Then we're going to talk about the `Cluster Proportional Autoscaler`, which actually allows you to expand the cluster with a proportional number of pods or applications attached to each node in the cluster. And then last but not least, we're going to talk about `KEDA`, which is the `Kubernetes Event-Driven Autoscaling`. KEDA can basically trigger off of things like latency, other metrics, events, buses, things like Kafka or SQS. You can actually trigger KEDA off of events, and that's very custom. And we'll talk about that as well. But that's the course content that we're going to be focused on. A little bit of background around scaling. We're going to talk about the HPA, the VPA, the CPA, and we're talking about KEDA as well. So you're going to hear those acronyms a lot. 

HPA is Horizontal Pod Autoscaler, VPA is Vertical Pod Autoscaler, CPA is Cluster Proportional Autoscaler, and then there's KEDA. 

Now I do want to mention there is also Cluster Autoscalers, which is very different than Cluster Proportional Autoscalers because Cluster Autoscalers will expand the number of nodes and not necessarily the applications that sit on top of the cluster. So we're actually going to make reference to both just to make sure that we're covering our bases. There are tons of Cluster Autoscalers out there, including native ones for cloud providers or projects like Karpenter for AWS that will actually scale your cluster based on certain criteria. 

So key takeaways here are that you're going to walk away with a solid foundation in Kubernetes scaling by the time we're done. You're going to get some hands-on experience in implementing scaling both manually and using automation, and we're going to show you some best practices to really keep your applications running smoothly under fluctuating load. Sound like a plan? All right, let's gear up. Let's go have an informative and practical learning journey.

## Why do we need to Autoscale?

Hello everyone and welcome. Today we're going to talk about a topic that's increasingly crucial in the tech world. It's like, why do we need to autoscale? We're going to dive into this presentation. And when we do this, I want you to think about when the last time your application experienced a sudden spike in demand, a sudden spike in a capacity need. How did it handle that surge? So let's explore how autoscaling can be a game changer in such scenarios. But first, let's talk about it. 

So one, why autoscaling Kubernetes, right? You ask, well, here's one compelling reason. And this one is namely `cost savings`. So imagine running a business where you're paying for a resource and you're not using everything. That's not ideal, right? So autoscaling helps prevent waste by basically dynamically adjusting resources based on current demand. Cost savings is just kind of the tip of the iceberg. There's a lot more to uncover about autoscaling, especially when it comes to not only maintaining underscaling, like making sure that you're staying in line with customer needs, but basically how to maintain optimal performance in Kubernetes environments. Let's dive a little deeper into that, because it's not just cost savings. It's just not about providing the exact resources exactly when you need them. 

But now that we've touched on cost savings, we've also got to talk about `improving application availability`. Autoscaling ensures your application is going to handle varied workloads seamlessly. Picture a sudden surge in users during a flash sale or some kind of product launch. And imagine that you want them to have that scale without basically experiencing any kind of hiccups. So autoscaling is the service that steps in to automatically scale application resources, making sure that your users and customers have a smooth customer experience without hiccups. Now, that also means that when demand dips, it's going to scale down as well, that's exactly what we want. 

And so you not only get to save resources and costs, but it ties into basically effective and efficient resource management. So when we talk about `efficient resource utilization`, how many of us have seen systems crash because of resource shortage, or there's just tons of stuff lying idle. So that's what really autoscaling is after. It's basically about shrinking these demands by basically either increasing capacity or reducing capacity, depending on what the actual customer need is. It's all about striking the right balance, not too much, not too little. Think of us trying to stay in that kind of Goldilocks zone, if you're familiar with the tale, where we don't provide extra resources, but we also don't under-provision the resources, right? 

And so this really focuses on another buzzword that you find in the tech world that autoscaling kind of embodies, which is `elasticity`, right? This elasticity is this key characteristic, and arguably one of the reasons why things like cloud native, this includes both the cloud providers and Kubernetes itself, is so amazingly effective is that both from a virtual machine perspective, depending on how you provision your cluster, and how you allocate resources to the pods that contain your containerized applications, imagine that you could scale those up and down elastically as needed for when you needed the capacity, and when you didn't need the capacity. This is all without manual intervention, by the way. So this kind of elasticity allows your applications to adapt to unpredictable traffic patterns efficiently. And isn't that what flexibility is really about for every modern application, right? 

Let's talk about another benefit, `fault tolerance and recovery`. So fault tolerance and recovery are also two crucial elements for maintaining application reliability. And so autoscaling is going to help your system withstand failures by distributing the load and ensuring basically that if there's a failure, you can either recover quickly or not even see the failure. 

And so autoscaling can automatically recover interrupted resources, making sure that you have the necessary capacity. We also want to talk about `seamless load management`. So these all very much tie together. But this, again, is just going to make sure that as there's fluctuations in demand, things autoscale up and down as needed. 

The other piece probably is the best is that we're talking about `simplified automated management`. You are simplifying your operational overhead by basically creating a system that automatically scales up and down with minimal manual intervention. This is key. Autoscaling takes care of the manual heavy lifting by automatically adjusting the resources both in the cluster and for your applications, allowing your team to focus on other things rather than firefighting resource issues, especially when the solutions are known. Application doesn't have enough memory? Give it more memory. Has too much memory? Give it less memory, right? And this kind of simplification leads to increased productivity and efficiency for all kinds of teams. 

So let's talk about the mechanics of scaling with Kubernetes next. So how does scaling work in Kubernetes? What does that look like? So it primarily involves two aspects. One, there is `cluster scaling` where you are actually expanding the nodes, the virtual machines that are hosting the pods or the containerized applications. So that means your worker nodes are going to scale up and down. But it's very different from `pod scaling`, which sits on top of the cluster, where the pod scaling is going to focus on the application level, ensuring that you have the right number of copies of your application, like your web servers and your application servers, for example, to handle the load. Now, I do want to caveat that while we are talking about pod deployment, even stateful set scaling, remember that databases typically have some other complications, you can't just scale them up and down. But you could add replicas, couldn't you? Just so you know, if stateful applications in general have another dimension of complexity when it comes to scaling, but even those applications can be solved through pod scaling, right? 

So what exactly does cluster scaling involve? Let's unpack that next. Alright, cluster scaling. Okay, so cluster scaling involves `shifting the number of nodes of worker nodes that exist inside the cluster`. Now, I want to make a distinction here, this is different from the Cluster Proportional Autoscaler or the CPA, this is the Cluster Autoscaler. So I want to make sure there's another product called the Cluster Proportional Autoscaler. And you can actually just do that through daemon sets. Here, we're talking about the Cluster Autoscaler, right? And so the Cluster Autoscaler allows you basically to expand the worker nodes, which are often virtual machines that are inside your cluster. And this expands total capacity, CPU, memory, disk, even GPUs, if you have them attached to your virtual machines. And so what this does is this allows things like the Vertical Pod Autoscaler, or the Horizontal Pod Autoscaler to basically either resize or expand the resources that are provided by the Cluster Autoscaler. So think of this as a lower level of virtual machine scaling, which is basically what the worker nodes represent. It's not application scaling, think of this as operating system scaling. 

So if we look at this slightly differently, by the way, notice here that when we talk about pod scaling, we're talking about within one or more namespaces, having a certain number of applications running, right. And so they deserve very distinct purposes. Here, what we're doing is we're expanding either using the Horizontal Pod Autoscaler expanding the number, we're changing the size, or we're scaling based on events. And those are the things that we're going to talk about. By the end of this course, you're going to know what the difference is between Cluster Autoscaling, where you add more worker nodes versus Horizontal Pod Autoscaler, Vertical Pod Autoscaler, or basically the events autoscaler. That's what KEDA stands for. 

So why do we need different strategies, by the way, well, remember, cluster autoscaling is all about scaling nodes, making sure the cluster stays up and making sure the capacity of the cluster is expanded. Remember, your applications sit on top of this cluster of virtual machines. So then when we talk about pod scaling, we're not talking about expanding the number of operating systems we have in our cluster, we're talking about the applications that are sitting on top of this cluster. And so this is going to increase application availability, application efficiency, there's a bunch of components here. 

And so this is why we need different strategies, because one increases the capacity of the cluster. And the pod scaling increases the capacity of the applications inside the cluster. 

So pod scaling works at the workload level, cluster scaling works at the base level of the cluster. So they work hand in hand to make sure to optimize your Kubernetes environment, we're going to ensure it adapts to change while maintaining cost efficiency, resource availability, fault tolerance, all the things that we talked about. So let's consider the broader strategies here. In conclusion, the classification of scaling strategies in Kubernetes is fundamental for maintaining a balanced, efficient and resilient environment. So cluster scaling to `ensure that your infrastructure is up to the task`, while pod scaling is going to `guarantee the application is up to the task`. So both of these work together to make sure that your Kubernetes cluster both has the capacity that it needs, and that the applications have the capacity that it needs. So I hope this session has shed some light on the importance and mechanics of autoscaling Kubernetes. We are just getting started. So we will see you in the next video. 

## Manual HPA

Welcome, everyone, we're going to talk about the manual HPA lab that we're going to do. So you know, we've been diving into the world of Kubernetes. So this lab focuses on the kind of manual version of the Horizontal Pod Autoscaler, or the HPA for short, right, also known as horizontal pod autoscaling. So here, this makes total sense. Alright, so let's paint a picture. So first, we've got our cluster, and it's like bustling, it's got pods in it. It's like a city, it's got you know, applications with their own roles and functions. There's some deployments here. And inside our app, their applications are deployed inside of pods, right? Now our users are visiting these pods and getting outputs, right? So what's the objective then of this lab? Like, Michael, why are you talking about pods and applications and about user access? Okay, the objective is actually pretty straightforward. We're going to manually scale, basically how a deployment affects application behavior, right? Basically how all it also, you could have a host name change, right? We're going to basically show you what happens when we scale manually, a pod on a replica of pods inside Kubernetes, right? Think of it like we're going to give our neighborhood more houses, and we're going to see how the community grows. But there's going to be some changes. So we have to adapt to that. Second is that we're going to show the impact of doing that kind of duplication, also known as replication or replicas, right? We're going to take a look at that behavior. So we're going to basically show you how scaling a pod changes the application's output. That's the objective of this lab. 

So here, we're basically scaling our Flask web app up to three replicas, we're basically saying I need you to scale up. 

```bash
kubectl scale deployment flask-app --replicas=3
```

And the reason that's significant is that if we go in and look at the pods, because we've manually scaled it, what will happen is that you'll see now that there's three pods, and they have this unique name. Now we're doing that manually. This simply tells Kubernetes, by the way, to add more houses to our neighborhood, in this case, more applications to our deployment, think of the deployment as our neighborhood, right? So the point is going to scale and the new pods are going to start. So in this case, we actually scaled the deployment to three replicas or three pods. And so we looked at it, and this is now what we see. So this is what we're trying to do, basically, is we're trying, we're trying to show you what happens when we add another pod manually, what changes, how does that change the user access? Does it change the name of the pods? Because we want to show you how to do it manually before we show you later, how to do it automated, right? So here's some key takeaways, right? 

One, scaling deployments by increasing replicas is like, basically expanding our neighborhood with more homes. So we're adding more applications to the deployment. That's what replicas are, they are copies, right, of pods that basically have applications in them. And so our deployment has a certain number of pods in it, we are scaling that up and down. 

Second, you'll notice that every pod, which by the way, every pod contains an application, it contains a container as an application, it's got a unique host name, and that changes when we redeploy the pod. So this can affect the output of applications if we're trying to access it through a host name. That's not typically what we do, by the way. And if you know Kubernetes, you know that we don't typically go after pod host names, but they can change. 

And last is that scaling will change the capacity of applications, right? And so for example, if your application is dependent upon the host name, that can change things. If you have sticky sessions, and it's a web server, that can change things. So typically stateless is the application behavior that we want to see in our applications. So because you're going to scale up and down, these pods are going to be ephemeral. And so if you need persistent data, typically what we do is we externalize it either external to the Kubernetes cluster or use StatefulSets. And we have very persistent pods that maintain naming conventions. But for our purposes today, we just wanted you to see what manual scaling from a Horizontal Pod Autoscaler perspective, what that would look like if you just did it yourself.

## Manual VPA

Welcome, everyone. We're going to talk about a lab that we have for the Vertical Pod Autoscaler or the VPA, right? So let's talk about this overview. First and foremost, picture this, we got a cluster, we've got deployments on the cluster, right? This cluster is right at our fingertips, ready to be orchestrated. And what we're going to do is we're going to deploy a Flask application, a single replica, mind you, and experience firsthand how users can access the Flask application inside the container inside the pod. Yes, I did say that the application that's inside the container inside the pod. So we're going to consider what happens if we need to have more resources for that single replica. Alright, so we're going to apply deployment, and we're going to see our Flask app be configured, right? And then we're not going to mess necessarily with a service. But by applying the deployment files, something interesting is going to happen because we're going to go out there. And we're going to get our pod information. And we're going to notice that it looks like it's changed. Right? It's going to look like something different is happening, where all of a sudden, a pod is starting and another one is terminating. Notice three minutes and seconds on that second pod. So that's what we're doing for our lab is that we're actually upsizing the single pod that we have in our deployment. And so what's going to happen is it's going to go from whatever size it's at, to a bigger size, and the old one's going to go away. This is the Vertical Pod Autoscaler, it makes the application's capacity bigger. That's what it does. And so that's going to be the focus of this lab, right is that we're going to basically resize our existing application into a larger, like capacity set. So as a reminder, the VPA dynamically adjusts pod resources and sizing based on observed usage. We're going to do that manually. So you can understand what it would take. So think about how doing this in an automated fashion can be a game changer in Kubernetes.
